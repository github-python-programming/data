# coleta de dados
import requests
from parsel import Selector
import sidrapy

# manipulação dos dados
import pandas as pd
import numpy as np
import io
import zipfile
import unicodedata
import re
import json

# sistema
import os
import sys
import datetime
from time import sleep
import random
import shutil
import tempfile

# selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver import ActionChains

# github
from github import Github
from github import Auth

# ************************
# CONSTANTES
# ************************

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko) '
                         'Chrome/118.0.0.0 Safari/537.36'}

git_doc_path = 'https://raw.githubusercontent.com/github-python-programming/data/main/doc-links.json'

script_path = os.getcwd()
dir_name = 'to_github'
data_folder = 'data'
script_folder = 'script'
dir_path = os.path.join(script_path, dir_name, data_folder)


# ************************
# FUNÇÕES
# ************************

# adiciona certo delay às requisições
def delay_requests(maximum):
    sleep(random.random() * maximum)


# faz requisições aos sites
def open_url(path: str):
    r = requests.get(path, headers=headers)
    delay_requests(2)
    return r


# faz requisições aos site e passa o html para o selector de tags
def get_html(path: str):
    r = requests.get(path, headers=headers)
    sel = Selector(text=r.text)
    delay_requests(2)
    return sel


# abre os arquivos baixados
def open_file(file_path=None, ext=None, sep=',', skiprows=None, excel_name=None, sheet_name=None):
    if ext == 'csv' and file_path is not None:
        if not isinstance(file_path, str):
            dataframe = pd.read_csv(io.BytesIO(file_path), encoding='utf-8', sep=sep, decimal=',')
            return dataframe
        else:
            dataframe = pd.read_csv(file_path, encoding='utf-8', sep=sep, decimal=',')
            return dataframe

    elif ext == 'xls' and file_path is not None:
        if not isinstance(file_path, str):
            dataframe = pd.read_excel(io.BytesIO(file_path),
                                      decimal=',', sheet_name=sheet_name, skiprows=skiprows)
            return dataframe
        else:
            dataframe = pd.read_excel(file_path,
                                      decimal=',', sheet_name=sheet_name, skiprows=skiprows)
            return dataframe
    else:
        if not isinstance(file_path, str) and file_path is not None:
            with zipfile.ZipFile(io.BytesIO(file_path), 'r') as zfile:
                excel_tables = zfile.namelist()
                for e_tb in excel_tables:
                    if e_tb.startswith(excel_name):
                        excel_content = zfile.read(e_tb)
            dataframe = pd.read_excel(io.BytesIO(excel_content),
                                      decimal=',', sheet_name=sheet_name, skiprows=skiprows)
            return dataframe

        else:
            if file_path is not None:
                with zipfile.ZipFile(file_path, 'r') as zfile:
                    excel_tables = zfile.namelist()
                    for e_tb in excel_tables:
                        if e_tb.startswith(excel_name) and e_tb.endswith('.xlsx'):
                            excel_content = zfile.read(e_tb)
                dataframe = pd.read_excel(io.BytesIO(excel_content),
                                          decimal=',', sheet_name=sheet_name, skiprows=skiprows)
                return dataframe


# converte dataframes em arquivos csv
def to_csv(data_to_convert, data_name):
    data_to_convert.to_csv(os.path.join(dir_path, data_name),
                           encoding='utf-8', decimal=',', index=False)


# remove acentuação do título do gráfico
def remove_accent(title_txt):
    clean_text = unicodedata.normalize('NFKD', title_txt).encode('ASCII', 'ignore').decode('utf-8')
    return clean_text


# criação do diretório
if not os.path.exists(dir_path):
    os.mkdir(dir_name)
    os.mkdir(os.path.join(dir_name, data_folder))
    os.mkdir(os.path.join(dir_name, script_folder))

# # ************************
# # EXTRAÇÃO DE DADOS POR MÉTODO DE COLETA WEB SCRAPING
# # ************************
#
# # coleta de informações sobre os gráficos
# git_doc = open_url(git_doc_path)
# doc = pd.DataFrame(git_doc.json())
# doc_scraping = doc.loc[doc['method'] == 'scraping'].reset_index(drop=True)
#
# for fig, url in zip(doc_scraping['figure'], doc_scraping['url']):
#     if fig.startswith('Gráfico 7.1'):
#         print(f'A baixar o arquivo da url:\n {url} ...')
#
#         # downloading do arquivo
#         xpath = '/html/body/form/div[3]/div[2]/section/div[2]/div/span/div[2]/div[1]/div[5]/div/div/div/div[' \
#                 '2]/div/table/tbody/tr/td[2]/a/@href'
#         html = get_html(url)
#         html_urls = html.xpath(xpath).getall()
#         url_to_get = [item for item in html_urls if item.endswith('.xlsx') or item.endswith('.xls')][0]
#         url_to_get = 'https://www.epe.gov.br' + url_to_get
#         file = open_url(url_to_get)
#
#         # organização do arquivo
#         # seleção das colunas e das linhas de interesse
#         df = open_file(file.content, 'xls', sheet_name='Tabela 3.63', skiprows=8)
#         df = df.iloc[:, 1:12]
#         df = df.loc[df[' '].isin(['Brasil', 'Nordeste', 'Sergipe'])]
#
#         # reordenação da variável ano para o eixo y
#         # renomeação do rótulo da coluna de ' ' para 'Região'
#         # ordenação alfabética da coluna 'Região' e cronológica da coluna 'Ano'
#         df_melted = df.melt(id_vars=[' '], value_vars=[str(y) for y in np.arange(2013, 2023)], var_name=['Ano'],
#                             value_name='Valor')
#         df_melted.rename(columns={' ': 'Região'}, inplace=True)
#         df_melted.sort_values(by=['Região', 'Ano'], ascending=[True, True], inplace=True)
#
#         # classificação dos valores em cada coluna
#         df_melted[df_melted.columns[0]] = df_melted[df_melted.columns[0]].astype('str')
#         df_melted[df_melted.columns[1]] = pd.to_datetime(df_melted[df_melted.columns[1]], format='%Y')
#         df_melted[df_melted.columns[2]] = df_melted[df_melted.columns[2]].astype('float64')
#
#         # tratamento do título da figura para nomeação do arquivo csv
#         g_name = remove_accent(fig.split(':')[0].lower().replace(' ', '_').replace('.', '-') + '_')
#         to_csv(df_melted, g_name + 'epe-anuario-energia.csv')
#
#     elif fig.startswith('Gráfico 8.1'):
#         print(f'A baixar o arquivo da url:\n {url} ...')
#
#         # downloading do arquivo
#         xpath = '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div/ul[1]/li[2]/a/@href'
#         html = get_html(url)
#         url_to_get = html.xpath(xpath).get()
#         file = open_url(url_to_get)
#
#         # organização do arquivo
#         # dados sobre sergipe
#         # remoção de variáveis não utilizáveis
#         # adição da identificação da região
#         df = open_file(file.content, 'csv', ';')
#         df_se = df.loc[df['UNIDADE DA FEDERAÇÃO'] == 'SERGIPE']
#         df_se = df_se.drop(['GRANDE REGIÃO', 'UNIDADE DA FEDERAÇÃO', 'PRODUTO'], axis='columns')
#         df_se['REGIÃO'] = 'SERGIPE'
#
#         # adicionado após comentário das linhas acima
#         month_mapping = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6,
#                          'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}
#         df_concat = df_se
#         df_concat['month'] = df_concat['MÊS'].str.lower().map(month_mapping)
#         df_concat['date'] = df_concat['ANO'].astype('str') + '-' + df_concat['month'].astype('str')
#         df_concat['ANO'] = df_concat['date']
#         df_concat.drop(['month', 'date', 'MÊS'], axis='columns', inplace=True)
#
#         # classificação dos dados
#         df_concat[df_concat.columns[0]] = pd.to_datetime(df_concat[df_concat.columns[0]], format='%Y-%m')
#         df_concat[df_concat.columns[1]] = df_concat[df_concat.columns[1]].astype('str')
#         df_concat[df_concat.columns[2]] = df_concat[df_concat.columns[2]].astype('float64')
#         df_concat[df_concat.columns[-1]] = df_concat[df_concat.columns[-1]].astype('str')
#
#         # tratamento do título da figura para nomeação do arquivo csv
#         g_name = remove_accent(fig.split(':')[0].lower().replace(' ', '_').replace('.', '-') + '_')
#         to_csv(df_concat, g_name + 'anp_producao_petroleo.csv')
#
#     elif fig.startswith('Gráfico 8.2'):
#         print(f'A baixar o arquivo da url:\n {url} ...')
#
#         # downloading do arquivo
#         xpath = '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div/ul[3]/li[2]/a/@href'
#         html = get_html(url)
#         url_to_get = html.xpath(xpath).get()
#         file = open_url(url_to_get)
#
#         # dados sobre sergipe
#         # seleção das variáveis de interesse
#         # adição da variável 'REGIÃO'
#         df = open_file(file.content, 'csv', sep=';')
#         df_se = df.loc[df['UNIDADE DA FEDERAÇÃO'] == 'SERGIPE']
#         df_se = df_se.groupby(['ANO', 'MÊS', 'LOCALIZAÇÃO', 'PRODUTO'])['PRODUÇÃO'].sum().reset_index()
#         df_se['REGIÃO'] = 'SERGIPE'
#
#         df_gas = df_se
#
#         # downloading do segundo arquivo
#         xpath = '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div/ul[2]/li[2]/a/@href'
#         url_to_get = html.xpath(xpath).get()
#         file = open_url(url_to_get)
#
#         # dados sobre sergipe
#         # seleção das variáveis de interesse
#         # adição da variável 'LOCALIZAÇÃO'
#         # adição da variável 'REGIÃO'
#         df = open_file(file.content, 'csv', sep=';')
#         df_se = df.loc[df['UNIDADE DA FEDERAÇÃO'] == 'SERGIPE']
#         df_se = df_se.groupby(['ANO', 'MÊS', 'PRODUTO'])['PRODUÇÃO'].sum().reset_index()
#         df_se['LOCALIZAÇÃO'] = 'NÃO SE APLICA'
#         df_se['REGIÃO'] = 'SERGIPE'
#
#         df_lgn = df_se
#         df_lgn = df_lgn[['ANO', 'MÊS', 'LOCALIZAÇÃO', 'PRODUTO', 'PRODUÇÃO', 'REGIÃO']]
#
#         # união das tabelas de ambos os arquivos
#         # classificação dos dados
#         df_concat = pd.concat([df_gas, df_lgn], ignore_index=True)
#
#         # adicionado após comentários acima
#         month_mapping = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6,
#                          'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}
#         df_concat['month'] = df_concat['MÊS'].str.lower().map(month_mapping)
#         df_concat['date'] = df_concat['ANO'].astype('str') + '-' + df_concat['month'].astype('str')
#         df_concat['ANO'] = df_concat['date']
#         df_concat.drop(['month', 'date', 'MÊS'], axis='columns', inplace=True)
#
#         df_concat[df_concat.columns[0]] = pd.to_datetime(df_concat[df_concat.columns[0]], format='%Y-%m')
#         df_concat[df_concat.columns[1:3]] = df_concat[df_concat.columns[1:3]].astype('str')
#         df_concat[df_concat.columns[-2]] = df_concat[df_concat.columns[-2]].astype('float64')
#         df_concat[df_concat.columns[-1]] = df_concat[df_concat.columns[-1]].astype('str')
#
#         # tratamento do título da figura para nomeação do arquivo
#         # conversão em arquivo csv
#         g_name = remove_accent(fig.split(':')[0].lower().replace(' ', '_').replace('.', '-') + '_')
#         to_csv(df_concat, g_name + 'anp_producao_gas_e_lgn.csv')
#
# # ************************
# # EXTRAÇÃO DE DADOS POR MÉTODO DE COLETA API
# # IBGE CONTAS REGIONAIS, IBGE INDICADORES SOCIAIS, SICONFI TESOURO
# # ************************
#
# doc_api = doc.loc[doc['method'] == 'API'].reset_index(drop=True)
# api_url = set(doc_api['url'].to_list())
#
# for url in api_url:
#
#     # downloading dos arquivos com fonte: sistema de contas regionais
#     if url.endswith('Contas_Regionais'):
#         print(f'A baixar o arquivo da url:\n {url} ...')
#         response = open_url(url)
#         df = pd.DataFrame(response.json())
#
#         # pequisa pela publicação mais recente --> inicia com '2' e possui 4 caracteres
#         df = df.loc[(df['name'].str.startswith('2')) &
#                     (df['name'].str.len() == 4),
#                     ['name', 'path']].sort_values(by='name', ascending=False).reset_index(drop=True)
#
#         # obtém o caminho da publicação mais recente e adiciona à url de acesso aos arquivos
#         url_to_get = df['path'][0][-5:] + '/xls'
#         response = open_url(url + url_to_get)
#         df = pd.DataFrame(response.json())
#         url_to_get_pib = df.loc[df['name'].str.startswith('PIB_Otica_Renda'), 'url'].values[0]
#         url_to_get_esp = df.loc[(df['name'].str.startswith('Especiais_2010')) &
#                                 (df['name'].str.endswith('.zip')), 'url'].values[0]
#
#         # downloading e organização do arquivo pib pela ótica da renda
#         file = open_url(url_to_get_pib)
#         df = open_file(file.content, 'xls', skiprows=8)
#         tables = ['Tabela1', 'Tabela10', 'Tabela18']
#
#         mapping = {
#             'grafico_1-5': 'Salários',
#             'grafico_1-6': 'Contribuição social',
#             'grafico_1-7': 'Impostos sobre produto, líquidos de subsídios',
#             'grafico_1-8': 'Excedente Operacional Bruto (EOB) e Rendimento Misto (RM)'
#         }
#
#         # seleção das tabelas e componentes de interesse
#         for k, v in mapping.items():
#             dfs = []
#             for tb in tables:
#                 # seleção de linhas não vazias
#                 # renomeação da coluna
#                 df_tb = df[tb]
#                 df_tb = df_tb.iloc[:9]
#                 df_tb = df_tb.rename(columns={'Unnamed: 0': 'Componente'})
#
#                 # reordenação da variável ano para o eixo y
#                 # seleção das linhas e das colunas de interesse
#                 df_melted = pd.melt(df_tb, id_vars=['Componente'], value_vars=df_tb.columns[1:], var_name=['Ano'],
#                                     value_name='Valor')
#                 df_melted = df_melted.loc[(df_melted['Componente'] == v) &
#                                           (df_melted['Ano'].str.endswith('.1'))]
#
#                 # remoção do ".1" ao final dos valores de Ano
#                 # decorrentes da ordenação padrão das variáveis Ano como colunas
#                 df_melted.loc[:, 'Ano'] = df_melted.loc[:, 'Ano'].apply(lambda x: x[:-2])
#
#                 # adição da variável região
#                 df_melted['Região'] = 'Brasil' if tb.endswith('1') else ('Nordeste' if tb.endswith('10') else 'Sergipe')
#
#                 # classificação dos dados
#                 df_melted[df_melted.columns[0]] = df_melted[df_melted.columns[0]].astype('str')
#                 df_melted[df_melted.columns[1]] = pd.to_datetime(df_melted[df_melted.columns[1]])
#                 df_melted[df_melted.columns[2]] = df_melted[df_melted.columns[2]].astype('float64')
#                 df_melted[df_melted.columns[3]] = df_melted[df_melted.columns[3]].astype('str')
#
#                 dfs.append(df_melted)
#
#             # conversão para arquivo csv
#             df_concat = pd.concat(dfs, ignore_index=True)
#             to_csv(df_concat, k + '_ibge_pib_otica_renda.csv')
#
#         # downloading e organização do arquivo especiais 2010
#         file = open_url(url_to_get_esp)
#         df = open_file(file.content, 'zip', excel_name='tab07.xls', skiprows=4)
#
#         # seleção das tabelas de interesse
#         tables = ['Tabela7.1', 'Tabela7.10', 'Tabela7.18']
#         dfs = []
#         for tb in tables:
#             df_tb = df[tb]
#
#             # renomeação do coluna de 'Unnamed: 0' para 'Atividade'
#             # seleção das linhas de interesse
#             df_tb.rename(columns={'Unnamed: 0': 'Atividade'}, inplace=True)
#             df_tb = df_tb.iloc[2:23]
#
#             # reordenação da variável ano para o eixo y
#             df_melted = pd.melt(df_tb, id_vars=['Atividade'], value_vars=[y for y in np.arange(2010, 2021)],
#                                 var_name=['Ano'], value_name='Valor')
#
#             # classificação dos dados por setor econômico
#
#             '''
#             Originalmente, os dados referentes ao setor e às atividades estão armazenados na mesma coluna.
#             A cada ocorrência do valor 'Agropecuária', por exemplo, será coletado o seu índice,
#             ponto de início da seleção dos valores.
#             O índice, então, é somado à quantia de atividades deste setor (neste caso 3) mais 1,
#             ponto de término da seleção dos valores.
#             A mesma operação é realizada para os outros dois setores econômicos.
#             '''
#
#             df_melted['Setor'] = ''
#
#             agro_index = df_melted.loc[df_melted['Atividade'] == 'Agropecuária'].index
#             ind_index = df_melted.loc[df_melted['Atividade'] == 'Indústria'].index
#             serv_index = df_melted.loc[df_melted['Atividade'] == 'Serviços'].index
#
#             for agro, ind, serv in zip(agro_index, ind_index, serv_index):
#                 df_melted.iloc[agro:agro + 4, -1] = 'Agropecuária'
#                 df_melted.iloc[ind:ind + 5, -1] = 'Indústria'
#                 df_melted.iloc[serv:serv + 12, -1] = 'Serviços'
#
#             # remoção dos valores referentes ao setor da coluna 'Atividade'
#             # adição da variável região
#             df_melted.drop(list(agro_index) + list(ind_index) + list(serv_index), axis='index', inplace=True)
#             df_melted['Região'] = 'Brasil' if tb.endswith('7.1') else ('Nordeste' if tb.endswith('7.10') else 'Sergipe')
#
#             dfs.append(df_melted)
#
#         # união dos dfs
#         # reordenação das colunas
#         df_concat = pd.concat(dfs, ignore_index=True)
#         df_concat = df_concat[['Região', 'Setor', 'Atividade', 'Ano', 'Valor']]
#
#         # classificação dos dados
#         df_concat[df_concat.columns[0:3]] = df_concat[df_concat.columns[0:3]].astype('str')
#         df_concat[df_concat.columns[-2]] = pd.to_datetime(df_concat[df_concat.columns[-2]], format='%Y')
#         df_concat[df_concat.columns[-1]] = df_concat[df_concat.columns[-1]].astype('float64')
#
#         # conversão em arquivo csv
#         to_csv(df_concat, 'tabela_1-1_ibge_especiais.csv')
#
#     elif url.endswith('Sintese_de_Indicadores_Sociais'):
#         print(f'A baixar o arquivo da url:\n {url} ...')
#         response = open_url(url)
#
#         # verifica a última publicação
#         url_to_last_pub = url + '/' + response.json()[-2]['path'].split('/')[-1] + '/xls'
#         response = open_url(url_to_last_pub)
#
#         # coleta o link do arquivo
#         url_to_file = response.json()[0]['url']
#         file = open_url(url_to_file)
#
#         # seleção das abas de interesse
#         df = open_file(file.content, 'zip', excel_name='Tabela 1.40', skiprows=6)
#
#         # seleção das abas de interesse
#         tables = []
#         for table in df.keys():
#             if not table.endswith('(CV)'):
#                 tables.append(table)
#
#         # união das tabelas de interesse
#         dfs = []
#         for tb in tables:
#             # seleção das linhas e colunas de interesse
#             # renomeação da coluna de 'Unnamed: 0' para 'Região' e remoção do .1 dos rótulos das colunas,
#             # decorrentes da repetição de valores como rótulos
#             df_tb = df[tb]
#             df_tb = df_tb.iloc[2:35, [0] + list(np.arange(5, 9))]
#             df_tb.columns = ['Região'] + [col[:-2] for col in df_tb.columns if col != 'Unnamed: 0']
#
#             # reordenação da variável ocupação para o eixo y
#             df_melted = pd.melt(df_tb, id_vars=['Região'], value_vars=df_tb.columns[1:],
#                                 var_name=['Ocupação'], value_name='Valor')
#
#             # dados sergipe, nordeste e brasil
#             # ordenação dos valores
#             # adição da variável ano
#             # df_melted = df_melted.loc[df_melted['Região'].isin(['Brasil', 'Nordeste', 'Sergipe'])]
#             df_melted = df_melted.loc[df_melted['Região'] == 'Sergipe']
#             df_melted = df_melted.sort_values(by=['Região', 'Ocupação'], ascending=[True] * 2)
#             df_melted['Ano'] = pd.to_datetime(tb, format='%Y')
#
#             dfs.append(df_melted)
#
#         # união dos dfs
#         # ordenação dos valores a partir das variáveis 'Região', 'Ano' e 'Ocupação'
#         # classificação dos dados
#         df_concat = pd.concat(dfs, ignore_index=True)
#         df_concat.sort_values(by=['Região', 'Ano', 'Ocupação'], ascending=[True] * 3, inplace=True)
#         df_concat[df_concat.columns[:2]] = df_concat[df_concat.columns[:2]].astype('str')
#         df_concat[df_concat.columns[-2]] = df_concat[df_concat.columns[-2]].astype('float64')
#
#         # conversão em arquivo csv
#         to_csv(df_concat, 'grafico_13-8_ibge_indicadores_sociais.csv')
#
#     elif url.startswith('https://apidatalake.tesouro.gov.br'):
#         print(f'A baixar o arquivo da url:\n {url} ...')
#
#         # definição dos anos de referência
#         base_year = 2015
#         current_year = datetime.datetime.now().year
#
#         dfs = []
#         for y in range(base_year, current_year + 1):
#
#             # coleta de dados a nível estadual ------------------------------------------------------------------------
#             siconfi_url = f"https://apidatalake.tesouro.gov.br/ords/siconfi/tt/rgf?an_exercicio={y}&" \
#                           f"in_periodicidade=Q&nr_periodo=3&co_tipo_demonstrativo=RGF&no_anexo=RGF-Anexo%2002&" \
#                           f"co_esfera=E&co_poder=E&id_ente=28"
#
#             response = open_url(siconfi_url)
#             if response.status_code == 200 and len(response.json()['items']) > 1:
#                 print(f'A coletar dados do siconfi referentes ao exercício de {y} ...\nNível regional: Sergipe')
#
#                 # seleção das colunas de interesse
#                 # seleção do quadrimestre de interesse
#                 # seleção das contas de interesse
#                 df = pd.DataFrame(response.json()['items'])
#                 df = df.loc[:, ['exercicio', 'instituicao', 'uf', 'coluna', 'conta', 'valor']]
#                 df = df.loc[df['coluna'] == 'Até o 3º Quadrimestre']
#                 df = df.loc[(df['conta'].str.startswith('DÍVIDA CONSOLIDADA LÍQUIDA (DCL)')) |
#                             (df['conta'].str.startswith('RECEITA CORRENTE LÍQUIDA - RCL')) |
#                             (df['conta'].str.startswith('% da DCL sobre a RCL'))]
#
#                 dfs.append(df)
#             else:
#                 print(f'Não foram encontrados dados referentes ao exercício de {y}!\nNível regional: Sergipe')
#
#         # união dos dfs
#         df_concat = pd.concat(dfs, ignore_index=True)
#
#         df_concat['conta'] = df_concat['conta'].apply(
#             lambda x: '% da DCL sobre a RCL' if x.startswith('% da DCL sobre a RCL') else (
#                 'DÍVIDA CONSOLIDADA LÍQUIDA' if x.startswith('DÍVIDA CONSOLIDADA LÍQUIDA') else
#                 'RECEITA CORRENTE LÍQUIDA'))
#
#         # ordenação dos valores
#         df_concat.sort_values(by=['uf', 'exercicio', 'conta'], ascending=[True] * 3, inplace=True)
#
#         # classificação dos dados
#         df_concat[df_concat.columns[0]] = pd.to_datetime(df_concat[df_concat.columns[0]], format='%Y')
#         df_concat[['conta', 'uf']] = df_concat[['conta', 'uf']].astype('str')
#         df_concat['valor'] = df_concat['valor'].astype('float')
#
#         # conversão em arquivo csv
#         to_csv(df_concat, 'grafico_11-11_siconfi_tesouro.csv')
#
# # ************************
# # EXTRAÇÃO DE DADOS POR MÉTODO DE COLETA API
# # TABELAS SIDRA
# # ************************
#
# # informações das figuras para o looping de requisições
# sidra_figures = {
#     'figure': ['grafico_4-2', 'grafico_5-4', 'grafico_13-6', 'tabela_13-2'],
#     'table': ['5603', '1761', '6402', '5434'],
#     'variable': ['631,706', '631,1243', '4099', '4090,4108'],
#     'class': ['', '', '86', '888'],
#     'class_val': ['', '', '95251', '47947,47948,47949,47950,56622,56623,56624,60032']
# }
#
# # códigos nos níveis regionais
# # regions = [('1', 'all'), ('2', '2'), ('3', '28')]
#
# df_sidra = pd.DataFrame(sidra_figures)
#
# # looping de requisições para cada figura
# for i in df_sidra.index:
#     print(f'A baixar tabelas do(a) {df_sidra["figure"][i]}')
#     # looping das figuras que não dispõem de classificação
#     if df_sidra['class'][i] == '':
#         regions = [('3', '28')]
#         dfs = []
#         # looping de requisições para cada tabela da figura
#         for reg in regions:
#             data = sidrapy.get_table(table_code=df_sidra['table'][i], territorial_level=reg[0],
#                                      ibge_territorial_code=reg[1],
#                                      variable=df_sidra['variable'][i], period="all")
#
#             # remoção da linha 0, dados para serem usados como rótulos das colunas
#             # não foram usados porque variam de acordo com a tabela
#             # seleção das colunas de interesse
#             data.drop(0, axis='index', inplace=True)
#             data = data[['D1N', 'D2N', 'D3N', 'V']]
#             dfs.append(data)
#
#             # acrescenta delay às requests
#             delay_requests(2)
#
#         # união dos dfs
#         # renomeação das colunas
#         # filtragem de dados a partir do ano 2010
#         df_concat = pd.concat(dfs, ignore_index=True)
#         df_concat.columns = ['Região', 'Ano', 'Variável', 'Valor']
#         df_concat = df_concat.loc[df_concat['Ano'] >= '2010']
#
#         # classificação dos dados
#         df_concat[['Região', 'Variável']] = df_concat[['Região', 'Variável']].astype('str')
#         df_concat['Ano'] = pd.to_datetime(df_concat['Ano'], format='%Y')
#         df_concat['Valor'] = df_concat['Valor'].astype('int64')
#
#         # conversão em arquivo csv
#         to_csv(df_concat, df_sidra['figure'][i] + f'_sidra_tb{df_sidra["table"][i]}.csv')
#
#     else:
#         # looping das figuras que dispõem de classificação
#         if df_sidra['figure'][i] != 'tabela_13-2':
#             regions = [('1', 'all'), ('2', '2'), ('3', '28')]
#             dfs = []
#             for reg in regions:
#                 data = sidrapy.get_table(table_code=df_sidra['table'][i], territorial_level=reg[0],
#                                          ibge_territorial_code=reg[1],
#                                          variable=df_sidra['variable'][i],
#                                          classifications={df_sidra['class'][i]: df_sidra['class_val'][i]}, period="all")
#
#                 # remoção da linha 0, dados para serem usados como rótulos das colunas
#                 # não foram usados porque variam de acordo com a tabela
#                 # seleção das colunas de interesse
#                 data.drop(0, axis='index', inplace=True)
#                 data = data[['D1N', 'D2N', 'D3N', 'D4N', 'V']]
#                 dfs.append(data)
#
#                 # acrescenta delay às requests
#                 delay_requests(2)
#
#             # união dos dfs
#             # renomeação das colunas
#             # filtragem de dados referentes ao 4º trimestre de cada ano
#             # seleção dos dígitos referentes ao ano
#             df_concat = pd.concat(dfs, ignore_index=True)
#             df_concat.columns = ['Região', 'Ano', 'Variável', 'Classe', 'Valor']
#             df_concat = df_concat.loc[df_concat['Ano'].str.startswith('4º trimestre')]
#             df_concat['Ano'] = df_concat['Ano'].apply(lambda x: x[-4:])
#
#             # classificação dos dados
#             df_concat[df_concat.columns[:-1]] = df_concat[df_concat.columns[:-1]].astype('str')
#             df_concat['Valor'] = df_concat['Valor'].replace('...', '0.0')
#             df_concat['Valor'] = df_concat['Valor'].astype('float64')
#             df_concat['Ano'] = pd.to_datetime(df_concat['Ano'], format='%Y')
#
#             # conversão em arquivo csv
#             to_csv(df_concat, df_sidra['figure'][i] + f'_sidra_tb{df_sidra["table"][i]}.csv')
#
#         else:
#             regions = [('3', '28')]
#             dfs = []
#             for reg in regions:
#                 data = sidrapy.get_table(table_code=df_sidra['table'][i], territorial_level=reg[0],
#                                          ibge_territorial_code=reg[1], variable=df_sidra['variable'][i],
#                                          classifications={df_sidra['class'][i]: df_sidra['class_val'][i]},
#                                          period="all")
#
#                 # remoção da linha 0, dados para serem usados como rótulos das colunas
#                 # não foram usados porque variam de acordo com a tabela
#                 # seleção das colunas de interesse
#                 data.drop(0, axis='index', inplace=True)
#                 data = data[['MN', 'D1N', 'D2N', 'D3N', 'D4N', 'V']]
#
#                 # separação de valores; valores inteiros e percentuais estão armazenados na mesma coluna
#                 data_ab = data.loc[data['MN'] == 'Mil pessoas']
#                 data_per = data.loc[data['MN'] == '%']
#                 data = data_ab.iloc[:, 1:]
#                 data['Percentual'] = data_per.loc[:, 'V'].to_list()
#                 dfs.append(data)
#
#                 # acrescenta delay às requests
#                 delay_requests(2)
#
#             # união dos dfs
#             # renomeação das colunas
#             # filtragem de dados referentes ao 4º trimestre de cada ano
#             df_concat = pd.concat(dfs, ignore_index=True)
#             df_concat.columns = ['Região', 'Ano', 'Variável', 'Atividade', 'Valor', 'Percentual']
#             df_concat = df_concat.loc[df_concat['Ano'].str.startswith('4º trimestre')]
#             df_concat['Ano'] = df_concat['Ano'].apply(lambda x: x[-4:])
#             df_concat.drop('Variável', axis='columns', inplace=True)
#
#             # classificação dos dados
#             df_concat[df_concat.columns[:-2]] = df_concat[df_concat.columns[:-2]].astype('str')
#             df_concat[df_concat.columns[-2:]] = df_concat[df_concat.columns[-2:]].astype('float64')
#             df_concat['Ano'] = pd.to_datetime(df_concat['Ano'], format='%Y')
#
#             # conversão em arquivo csv
#             to_csv(df_concat, df_sidra['figure'][i] + f'_sidra_tb{df_sidra["table"][i]}.csv')
#
#
# # ************************
# # TABELA 17.2
# # ************************
#
# temp_dir = tempfile.mkdtemp()
# # download da última publicação
# url = 'https://www.gov.br/inep/pt-br/areas-de-atuacao/pesquisas-estatisticas-e-indicadores/ideb/resultados'
# print(f'A baixar o arquivo da url:\n {url} ...')
#
# # acesso ao site com a aplicação selenium
# os.mkdir(os.path.join(temp_dir, 'last_pub'))
# options = webdriver.ChromeOptions()
# prefs = {'download.default_directory': os.path.join(temp_dir, 'last_pub')}
# options.add_experimental_option('prefs', prefs)
# options.add_argument('--headless=new')
#
# browser = webdriver.Chrome(options=options)
# browser.get(url)
#
# # encontra o botão de login e o fecha
# login_btn = browser.find_element(By.ID, 'govbr-login-overlay-wrapper')
# browser.implicitly_wait(2)
# ActionChains(browser).move_to_element(login_btn).click(login_btn).perform()
# browser.implicitly_wait(2)
#
# # encontra o botão dos cookies e aceita
# cookies_btn = browser.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div/div[2]/button[2]')
# browser.implicitly_wait(2)
# cookies_btn.click()
#
# # encontra o ano da última publicação
# year_element = browser.find_element(By.XPATH, '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div[1]/div[1]/div[1]')
# last_year = year_element.text
#
# # encontra o elemento que armazena o link de download do arquivo com dados a nível estadual e macrorregional
# link_element = browser.find_element(By.XPATH, '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div[2]/div[1]/div/div['
#                                               '2]/div/div/div/div[2]/ul/li/a[1]')
# link = link_element.get_attribute('href')
# browser.implicitly_wait(2)
#
# # baixa o arquivo; sergipe e nordeste
# browser.get(link)
# sleep(3)
# browser.quit()
#
# # download das publicações anteriores
# # nova inicialização do browser para definir pasta padrão de download
# os.mkdir(os.path.join(temp_dir, 'old_pub'))
# options = webdriver.ChromeOptions()
# prefs = {'download.default_directory': os.path.join(temp_dir, 'old_pub')}
# options.add_experimental_option('prefs', prefs)
# options.add_argument('--headless=new')
#
# browser = webdriver.Chrome(options=options)
# browser.get(url)
# login_btn = browser.find_element(By.ID, 'govbr-login-overlay-wrapper')
# browser.implicitly_wait(2)
# ActionChains(browser).move_to_element(login_btn).click(login_btn).perform()
# browser.implicitly_wait(2)
# cookies_btn = browser.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div/div[2]/button[2]')
# browser.implicitly_wait(2)
# cookies_btn.click()
# browser.implicitly_wait(2)
#
# # altera para a página das publicações anteriores
# next_link_element = browser.find_element(By.XPATH, '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div[1]/div['
#                                                    '1]/div[2]/a')
# browser.implicitly_wait(2)
# ActionChains(browser).move_to_element(next_link_element).click(next_link_element).perform()
#
# # encontra o elemento que armazena o link de download do arquivo com dados a nível estadual e macrorregional
# link_element = browser.find_element(By.XPATH, '/html/body/div[2]/div[1]/main/div[2]/div/div[4]/div[2]/div[2]/div/div['
#                                               '2]/div/div/div/div[2]/ul/li/a')
# link = link_element.get_attribute('href')
# browser.implicitly_wait(2)
#
# # baixa o arquivo; sergipe e nordeste
# browser.get(link)
# sleep(3)
# browser.quit()
#
# # coleta dos dados das últimas planilhas publicadas
# dfs_last = []
# for zip_file in os.listdir(os.path.join(temp_dir, 'last_pub')):
#     df = open_file(os.path.join(temp_dir, 'last_pub', zip_file), 'zip',
#                    excel_name='divulgacao_regioes_ufs_ideb', skiprows=9)
#
#     # looping para extrair dados de cada aba da planilha
#     dfs = []
#     for i, tb in enumerate(df.keys()):
#
#         # seleção das linhas e colunas de interesse
#         df_tb = df[tb]
#         df_tb = df_tb.loc[df_tb[df_tb.columns[0]] == 'Sergipe',
#                               df_tb.iloc[:, [0, 1, -1]].columns]
#
#         # indicação da série com base na aba aberta
#         # indicação do ano com base no valor do site; a última publicação contém dados de apenas um período anual
#         # indicação do tipo do dado; as últimas publicações dispõem apenas de dados, não há projeções
#         df_tb['Série'] = 'Fundamental - Anos Iniciais' if i == 0 else (
#             'Fundamental - Anos Finais' if i == 1 else 'Ensino Médio')
#         df_tb['Ano'] = last_year
#         df_tb['Classe'] = 'IDEB'
#
#         # renomeação das colunas
#         # reordenação das colunas
#         df_tb.columns = ['Região', 'Rede', 'Valor', 'Série', 'Ano', 'Classe']
#         df_tb = df_tb[['Ano', 'Região', 'Série', 'Rede', 'Classe', 'Valor']]
#
#         dfs.append(df_tb)
#
#     # união dos dados de cada aba da planilha
#     df_concat = pd.concat(dfs, ignore_index=True)
#     dfs_last.append(df_concat)
#
# # união dos dados da última publicações
# df_last = pd.concat(dfs_last, ignore_index=True)
#
#
# # coleta dos dados das planilhas publicadas anteriormente
# dfs_old = []
# for zip_file in os.listdir(os.path.join(temp_dir, 'old_pub')):
#
#     # dados sergipe e nordeste
#     if not 'brasil' in zip_file:
#         df = open_file(os.path.join(temp_dir, 'old_pub', zip_file), 'zip',
#                        excel_name='divulgacao_regioes_ufs_ideb', skiprows=9)
#     else:
#         df = open_file(os.path.join(temp_dir, 'old_pub', zip_file), 'zip',
#                        excel_name='divulgacao_brasil_ideb', skiprows=9)
#
#     # looping para extrair dados de cada aba da planilha
#     dfs = []
#     for i, tb in enumerate(df.keys()):
#
#         # seleção das linhas e colunas de interesse
#         # indicação da série com base na aba aberta
#         df_tb = df[tb]
#         if not 'brasil' in zip_file:
#             # df_tb = df_tb.loc[df_tb[df_tb.columns[0]].isin(['Sergipe', 'Nordeste']), df_tb.iloc[:, [0, 1] + list(
#             #     np.arange(-16, -0))].columns]
#             df_tb = df_tb.loc[df_tb[df_tb.columns[0]] == 'Sergipe', df_tb.iloc[:, [0, 1] + list(
#                 np.arange(-16, -0))].columns]
#         else:
#             df_tb = df_tb.loc[
#                 df_tb[df_tb.columns[0]] == 'Brasil', df_tb.iloc[:, [0, 1] + list(np.arange(-16, -0))].columns]
#         df_tb['Série'] = 'Fundamental - Anos Iniciais' if i == 0 else (
#             'Fundamental - Anos Finais' if i == 1 else 'Ensino Médio')
#
#         # renomeação das colunas para indicação do ano a que se refere o dado, para posterior tratamento
#         # índice IDEB
#         base_year = 2005
#         for col in df_tb[df_tb.columns[-17:-9]].columns:
#             df_tb.rename(columns={col: 'IDEB ' + str(base_year)}, inplace=True)
#             base_year += 2
#         # projeções
#         base_year = 2007
#         for col in df_tb[df_tb.columns[-9:-1]].columns:
#             df_tb.rename(columns={col: 'Projeção ' + str(base_year)}, inplace=True)
#             base_year += 2
#
#         # reorganização da tabela
#         df_melted = pd.melt(df_tb, id_vars=list(df_tb.iloc[:, [0, 1, -1]].columns),
#                             value_vars=list(df_tb.columns[2:-1]), var_name=['var'], value_name='val')
#         df_melted['Classe'] = df_melted['var'].apply(lambda x: x[:-5])
#         df_melted['Ano'] = df_melted['var'].apply(lambda x: x[-4:])
#         df_melted.drop('var', axis='columns', inplace=True)
#
#         # renomeação das colunas
#         # reordenação das colunas
#         df_melted.columns = ['Região', 'Rede', 'Série', 'Valor', 'Classe', 'Ano']
#         df_melted = df_melted[['Ano', 'Região', 'Série', 'Rede', 'Classe', 'Valor']]
#
#         dfs.append(df_melted)
#
#     # união dos dados de cada aba da planilha
#     df_concat = pd.concat(dfs, ignore_index=True)
#     dfs_old.append(df_concat)
#
# # união dos dados das publicações anteriores
# df_old = pd.concat(dfs_old, ignore_index=True)
#
# # união de todas as publicações
# df_united = pd.concat([df_last, df_old], ignore_index=True)
# df_united.sort_values(by=['Ano', 'Região', 'Classe', 'Série'], ascending=[True] * 4, inplace=True)
#
# # classificação dos dados
# df_united[df_united.columns[0]] = pd.to_datetime(df_united[df_united.columns[0]], format='%Y')
# df_united[df_united.columns[1:-1]] = df_united[df_united.columns[1:-1]].astype('str')
# df_united[df_united.columns[-1]] = df_united[df_united.columns[-1]].astype('float64')
#
# # coversão em arquivo csv
# to_csv(df_united, 'tabela_17-2_inep_ideb.csv')
#
# shutil.rmtree(temp_dir)
#
#
# # ************************
# # GRÁFICO 18.6
# # ************************
# temp_dir = tempfile.mkdtemp()
# url = 'http://tabnet.datasus.gov.br/cgi/deftohtm.exe?sih/cnv/nruf.def'
# print(f'A baixar o arquivo da url:\n {url} ...')
#
# # configurações do driver
# options = webdriver.ChromeOptions()
# prefs = {'download.default_directory': temp_dir,
#          "download.prompt_for_download": False,
#          "download.directory_upgrade": True,
#          "safebrowsing.enabled": True}
# options.add_experimental_option('prefs', prefs)
# options.add_argument('--headless=new')
#
# browser = webdriver.Chrome(options=options)
# browser.get(url)
# browser.implicitly_wait(2)
#
# # seleção de botões e elementos da páginas
# lin_btn = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[2]/div/div[1]/select/option[2]')
# ActionChains(browser).move_to_element(lin_btn).click(lin_btn).perform()
# browser.implicitly_wait(2)
#
# col_btn = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[2]/div/div[2]/select/option[7]')
# ActionChains(browser).move_to_element(col_btn).click(col_btn).perform()
# browser.implicitly_wait(2)
#
# cont_btn = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[2]/div/div[3]/select/option[15]')
# ActionChains(browser).move_to_element(cont_btn).click(cont_btn).perform()
# browser.implicitly_wait(2)
#
# per_btn_1 = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[3]/div/select/option[1]')
# ActionChains(browser).move_to_element(per_btn_1).click(per_btn_1).perform()
# browser.implicitly_wait(2)
#
# per_btn_2 = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[3]/div/select/option[189]')
# ActionChains(browser).key_down(Keys.SHIFT).click(per_btn_2).key_up(Keys.SHIFT).perform()
# browser.implicitly_wait(2)
#
# option_btn = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[4]/div[2]/div[1]/div[1]/input[1]')
# ActionChains(browser).move_to_element(option_btn).click(option_btn).perform()
# browser.implicitly_wait(2)
#
# show_btn = browser.find_element(By.XPATH, '/html/body/div/div/center/div/form/div[4]/div[2]/div[2]/input[1]')
# ActionChains(browser).move_to_element(show_btn).click(show_btn).perform()
# browser.implicitly_wait(2)
#
# windows = browser.window_handles
# browser.switch_to.window(windows[-1])
# print('A esperar o carregamento da página ...')
# download_btn = WebDriverWait(browser, 600).until(
#     EC.presence_of_element_located((By.XPATH, '/html/body/div/div/div[3]/table/tbody/tr/td[1]/a'))
# )
# print('Página carregada!')
# ActionChains(browser).move_to_element(download_btn).click(download_btn).perform()
# browser.implicitly_wait(2)
#
# # abertura e organização da tabela
# f_path = os.listdir(temp_dir)[0]
#
# df = pd.read_csv(os.path.join(temp_dir, f_path), sep=';', encoding='cp1252', skiprows=3, decimal=',')
#
# df_melted = df.melt(id_vars=[df.columns[0]], value_vars=list(df.columns[1:-1]), var_name=['Ano'], value_name='Valor')
# df_melted[df_melted.columns[0]] = df_melted[df_melted.columns[0]].str.replace(r'^\.\. ', '', regex=True)
#
# df_melted = df_melted.loc[df_melted[df_melted.columns[0]].isin(['Total', 'Região Nordeste', 'Sergipe'])]
# df_melted[df_melted.columns[0]] = df_melted[df_melted.columns[0]].replace('Total', 'Brasil')
#
# df_melted[df_melted.columns[0]] = df_melted[df_melted.columns[0]].astype('str')
# df_melted[df_melted.columns[1]] = pd.to_datetime(df_melted[df_melted.columns[1]], format='%Y')
# df_melted[df_melted.columns[2]] = df_melted[df_melted.columns[2]].astype('float64')
#
# to_csv(df_melted, 'grafico_18-6_tabnet_morbidade.csv')
#
# shutil.rmtree(temp_dir)
# browser.quit()
#
# # ************************
# # GRÁFICO 19.2
# # ************************
#
# url = 'https://forumseguranca.org.br/anuario-brasileiro-seguranca-publica/'
# print(f'A baixar o arquivo da url:\n {url} ...')
#
# html = get_html(url)
# to_get = html.xpath('/html/body/div[1]/section[3]/div/div[1]/div/section[2]/div/div/div/section/div/div['
#                     '2]/div/div/div/a/@href').get()
# file = open_url(to_get)
#
# df = open_file(file.content, 'xls', sheet_name='T03', skiprows=6)
#
# # remoção de linhas vazias
# # seleção das linhas de interesse
# df.dropna(axis='index', inplace=True)
# df = df.loc[df['Unnamed: 0'].isin(['Brasil', 'Região Nordeste', 'Sergipe'])]
#
# # renomeação da coluna sem rótulo
# # reorganização da tabela; variável ano passa para o eixo y
# df.rename(columns={'Unnamed: 0': 'Região'}, inplace=True)
# df_melted = pd.melt(df, id_vars=['Região'], value_vars=list(df.columns[1:]),
#                     var_name=['Ano'], value_name='Valor')
#
# # reordenação das variáveis
# # classificação dos dados
# df_melted.sort_values(by=['Região', 'Ano'], ascending=[True] * 2, inplace=True)
# df_melted[df_melted.columns[0]] = df_melted[df_melted.columns[0]].astype('str')
# df_melted[df_melted.columns[1]] = pd.to_datetime(df_melted[df_melted.columns[1]], format='%Y')
# df_melted[df_melted.columns[2]] = df_melted[df_melted.columns[2]].astype('int64')
#
# # conversão em arquivo csv
# to_csv(df_melted, 'grafico_19-2_anuario_seguranca_publica.csv')
#
#
# # ************************
# # GRÁFICO 19.12
# # ************************
#
# url = 'https://dados.mj.gov.br/dataset/sistema-nacional-de-estatisticas-de-seguranca-publica/resource/feeae05e-faba' \
#       '-406c-8a4a-512aec91a9d1'
# print(f'A baixar o arquivo da url:\n {url} ...')
#
# html = get_html(url)
# to_get = html.xpath('/html/body/div[2]/div/div[3]/section/div[1]/p/a/@href').get()
# file = open_url(to_get)
#
# df = open_file(file.content, 'xls', sheet_name='Ocorrências')
#
# # dados sergipe
# df_se = df.loc[(df['UF'] == 'Sergipe') & (df['Tipo Crime'] == 'Estupro')]
#
# # dados nordeste
# df_ne = df.loc[(df['UF'].isin(['Alagoas', 'Bahia', 'Ceará', 'Maranhão',
#                               'Paraíba', 'Pernambuco', 'Piauí', 'Rio Grande do Norte', 'Sergipe'])) &
#                (df['Tipo Crime'] == 'Estupro')]
# df_ne.iloc[:len(df_ne), 0] = 'Nordeste'
# df_ne = df_ne.groupby(['UF', 'Tipo Crime', 'Ano', 'Mês'])['Ocorrências'].sum().reset_index()
#
# # dados brasil
# df_br = df.loc[df['Tipo Crime'] == 'Estupro']
# df_br.iloc[:len(df_br), 0] = 'Brasil'
# df_br = df_br.groupby(['UF', 'Tipo Crime', 'Ano', 'Mês'])['Ocorrências'].sum().reset_index()
#
# # união dos dfs
# df_united = pd.concat([df_br, df_ne, df_se], ignore_index=True)
# df_united.rename(columns={'UF': 'Região'}, inplace=True)
#
# # classificação dos dados
# df_united[df_united.columns[:2]] = df_united[df_united.columns[:2]].astype('str')
# df_united[df_united.columns[2]] = pd.to_datetime(df_united[df_united.columns[2]], format='%Y')
# df_united[df_united.columns[3]] = df_united[df_united.columns[3]].astype('str')
# df_united[df_united.columns[-1]] = df_united[df_united.columns[-1]].astype('int64')
#
# # conversão em csv
# to_csv(df_united, 'grafico-19-12_sinesp_ocorrencias_criminais.csv')
#
#
# # ************************
# # TABELA 19.1
# # ************************
#
# url = 'https://dados.mj.gov.br/dataset/sistema-nacional-de-estatisticas-de-seguranca-publica/resource/feeae05e-faba' \
#       '-406c-8a4a-512aec91a9d1'
# print(f'A baixar o arquivo da url:\n {url} ...')
#
# html = get_html(url)
# to_get = html.xpath('/html/body/div[2]/div/div[3]/section/div[1]/p/a/@href').get()
# file = open_url(to_get)
#
# df = open_file(file.content, 'xls', sheet_name='Ocorrências')
#
# crimes = ['Roubo a instituição financeira', 'Roubo de carga', 'Roubo de veículo', 'Furto de veículo']
# ne_states = ['Alagoas', 'Bahia', 'Ceará', 'Maranhão',
#              'Paraíba', 'Pernambuco', 'Piauí', 'Rio Grande do Norte', 'Sergipe']
#
# # dados sergipe
# df_se = df.loc[(df['UF'] == 'Sergipe') & df['Tipo Crime'].isin(crimes)]
#
# # dados nordeste
# df_ne = df.loc[(df['UF'].isin(ne_states)) & (df['Tipo Crime'].isin(crimes))]
# df_ne.iloc[:len(df_ne), 0] = 'Nordeste'
# df_ne = df_ne.groupby(['UF', 'Tipo Crime', 'Ano', 'Mês'])['Ocorrências'].sum().reset_index()
#
# # dados brasil
# df_br = df.loc[df['Tipo Crime'].isin(crimes)]
# df_br.iloc[:len(df_br), 0] = 'Brasil'
# df_br = df_br.groupby(['UF', 'Tipo Crime', 'Ano', 'Mês'])['Ocorrências'].sum().reset_index()
#
# # união dos dfs
# df_united = pd.concat([df_br, df_ne, df_se], ignore_index=True)
# df_united.rename(columns={'UF': 'Região'}, inplace=True)
#
# # classificação dos dados
# df_united[df_united.columns[:2]] = df_united[df_united.columns[:2]].astype('str')
# df_united[df_united.columns[2]] = pd.to_datetime(df_united[df_united.columns[2]], format='%Y')
# df_united[df_united.columns[3]] = df_united[df_united.columns[3]].astype('str')
# df_united[df_united.columns[-1]] = df_united[df_united.columns[-1]].astype('int64')
#
# # conversão em csv
# to_csv(df_united, 'tabela_19-1_sinesp_ocorrencias_criminais.csv')
#
#
# # ************************
# # TABELA 19.2
# # ************************
#
# url = 'https://forumseguranca.org.br/anuario-brasileiro-seguranca-publica/'
# print(f'A baixar o arquivo da url:\n {url} ...')
#
# html = get_html(url)
# to_get = html.xpath('/html/body/div[1]/section[3]/div/div[1]/div/section[2]/div/div/div/section/div/div['
#                     '2]/div/div/div/a/@href').get()
# file = open_url(to_get)
#
# df = open_file(file.content, 'xls', sheet_name=None)
#
# # coleta de dados referentes à despesa per capita com segurança pública
# # seleção das linhas e colunas de interesse
# # renomeação das colunas
# df_despesas = df['G65']
# df_despesas = df_despesas.iloc[6:33, :2]
# df_despesas.columns = ['Região', 'Valor']
#
# # dados sergipe
# df_despesas_se = df_despesas.loc[df_despesas['Região'] == 'Sergipe']
#
# # dados nordeste
# ne_states = ['Alagoas', 'Bahia', 'Ceará', 'Maranhão', 'Paraíba',
#              'Pernambuco', 'Piauí', 'Rio Grande do Norte', 'Sergipe']
# df_despesas_ne = df_despesas.loc[df_despesas['Região'].isin(ne_states)]
# df_despesas_ne.iloc[:, 0] = 'Nordeste'
# df_despesas_ne = df_despesas_ne.groupby('Região')['Valor'].sum().reset_index()
#
# # dados brasil
# df_despesas_br = df_despesas.iloc[:]
# df_despesas_br.iloc[:, 0] = 'Brasil'
# df_despesas_br = df_despesas_br.groupby('Região')['Valor'].sum().reset_index()
#
# # união dos dfs
# # adição da variável ano
# # adição da variável classe
# df_despesas_united = pd.concat([df_despesas_br, df_despesas_ne, df_despesas_se], ignore_index=True)
# df_despesas_united['Ano'] = df['G65'].iloc[0, 0][-4:]
# df_despesas_united['Classe'] = 'Despesa per capita com Segurança Pública'
#
#
# # coleta de dados referentes à participação das despesas com segurança pública
# # renomeação das colunas
# # seleção das linhas de interesse
# # remoção de linhas vazias
# df_participacao = df['T55']
# df_participacao.columns = df_participacao.iloc[4]
# df_participacao = df_participacao.iloc[6:]
# df_participacao = df_participacao.dropna(axis='index')
#
# # reorganização da tabela; variável ano passada para o eixo y
# df_participacao_melted = df_participacao.melt(id_vars=df_participacao.columns[0],
#                                               value_vars=list(df_participacao.columns[1:]),
#                                               var_name=['Ano'], value_name='Valor')
# df_participacao_melted['Ano'] = df_participacao_melted['Ano'].astype('int64')
#
# # dados sergipe
# df_participacao_se = df_participacao_melted.loc[df_participacao_melted[df_participacao_melted.columns[0]] == 'Sergipe']
#
# # dados nordeste
# df_participacao_ne = df_participacao_melted.loc[
#     df_participacao_melted[df_participacao_melted.columns[0]].isin(ne_states)]
# df_participacao_ne.iloc[:, 0] = 'Nordeste'
# df_participacao_ne = df_participacao_ne.groupby([df_participacao_ne.columns[0],
#                                                 df_participacao_ne.columns[1]])['Valor'].mean().reset_index()
#
# # dados brasil
# df_participacao_br = df_participacao_melted.loc[df_participacao_melted[df_participacao_melted.columns[0]] == 'União']
# df_participacao_br.iloc[:, 0] = 'Brasil'
#
# # união dos dfs
# df_participacao_united = pd.concat([df_participacao_br, df_participacao_ne, df_participacao_se],
#                                    ignore_index=True)
# df_participacao_united['Classe'] = 'Participação das despesas com Segurança Pública'
# df_participacao_united = df_participacao_united.rename(columns={df_participacao_united.columns[0]: 'Região'})
#
#
# # coleta de dados referentes à razão preso/vaga
# # organização da tabela para seleção das linhas de interesse
# df_preso = df['T75']
# df_preso.columns = df_preso.iloc[5]
# df_preso = df_preso.dropna(axis='index')
# df_preso = df_preso.iloc[:, [0, -2, -1]]
# df_preso.columns = [str(int(col)) if not pd.isna(col) else 'Região' for col in df_preso.columns]
# df_preso = df_preso.melt(id_vars=['Região'], value_vars=list(df_preso.columns[1:]),
#                          var_name=['Ano'], value_name='Valor')
#
# # dados sergipe
# df_preso_se = df_preso.loc[df_preso['Região'] == 'Sergipe']
#
# # dados sergipe
# df_preso_ne = df_preso.loc[df_preso['Região'].isin(ne_states)]
# df_preso_ne.iloc[:, 0] = 'Nordeste'
# df_preso_ne = df_preso_ne.groupby(['Região', 'Ano'])['Valor'].mean().reset_index()
#
# # dados brasil
# df_preso_br = df_preso.loc[df_preso['Região'].str.startswith('Brasil')]
#
# # união dos dfs
# df_preso_united = pd.concat([df_preso_br, df_preso_ne, df_preso_se], ignore_index=True)
# df_preso_united['Classe'] = 'Razão preso/vaga'
#
#
# # união dos dfs de cada aba de interesse
# df_united = pd.concat([df_despesas_united, df_participacao_united, df_preso_united], ignore_index=True)
# df_united['Região'] = df_united['Região'].apply(lambda x: re.sub(r'\(\d+\)', '', x))
# df_united = df_united.sort_values(by=['Região', 'Ano', 'Classe'], ascending=[True]*3)
#
# # classificação dos dados
# df_united[df_united.columns[0]] = df_united[df_united.columns[0]].astype('str')
# df_united[df_united.columns[1]] = df_united[df_united.columns[1]].astype('float64')
# df_united[df_united.columns[2]] = pd.to_datetime(df_united[df_united.columns[2]], format='%Y')
# df_united[df_united.columns[3]] = df_united[df_united.columns[3]].astype('str')
#
# # conversão em arquivo csv
# to_csv(df_united, 'tabela_19-2_anuario_seguranca_publica.csv')


# ************************
# UPLOAD DE ARQUIVOS PARA REPOSITÓRIO NO GITHUB
# ************************

# definição do horário para registro de upload ou update dos arquivos
now = datetime.datetime.now().strftime('%d/%m/%Y, %H:%M:%S')

# caminhos de diretórios
# repo_path = 'anuariosocieconomico/T25'
repo_path = 'github-python-programming/data'
data_git_path = 'data'
script_git_path = 'script'
doc_git_path = 'doc'

# inicialização do repositório
# auth = Auth.Token('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')
auth = Auth.Token('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')
g = Github(auth=auth)
repo = g.get_repo(repo_path)
contents = repo.get_contents('')

# diretórios no git
git_cont = []
for content_file in contents:
    git_cont.append(content_file.path)

# verifica se há a pasta 'data' no diretório
if 'data' not in git_cont:
    print('Pasta não encontrada. A criar nova pasta no diretório ...')

    # upload dos arquivos csv
    csv_folder = 'to_github/data'
    for csv_file in os.listdir(csv_folder):
        csv_path = os.path.join(csv_folder, csv_file)

        if os.path.isfile(csv_path):
            with open(csv_path, 'r', encoding='utf-8') as file:
                csv_content = file.read()

            repo.create_file(f'data/{csv_file}', f'Arquivo criado em {now}.', csv_content)
            print(f'Arquivo {csv_file} criado no novo diretório.')
            sleep(1)
else:
    print('\nPasta encontrada. A atualizar os arquivos no diretório ...')

    # update dos arquivos csv
    my_folder = repo.get_contents(data_git_path)
    csv_folder = 'to_github/data'
    for csv_file in os.listdir(csv_folder):
        csv_path = os.path.join(csv_folder, csv_file)

        if os.path.isfile(csv_path):
            with open(csv_path, 'r', encoding='utf-8') as file:
                csv_content = file.read()

            file_in_folder = next((csv_f for csv_f in my_folder if csv_f.name == csv_file), None)

            # verifica se se arquivo local já existe no diretório, para definir se deve criá-lo ou atualizá-lo
            if file_in_folder:
                repo.update_file(f'Daniel/data/{csv_file}', f'Arquivo atualizado em {now}.',
                                 csv_content, file_in_folder.sha)
                print(f'Arquivo {csv_file} atualizado no novo diretório.')
                sleep(1)
            else:
                repo.create_file(f'Daniel/data/{csv_file}', f'Arquivo criado em {now}.', csv_content)
                sleep(1)

if 'script' not in git_cont:
    # upload do script
    script_path = 'get_data.py'
    with open(script_path, 'r', encoding='utf-8') as f:
        text = f.read()

    repo.create_file('script/script.txt', f'Arquivo criado em {now}', text)
    sleep(1)
else:
    # update do script
    my_folder = repo.get_contents(script_git_path)
    script_path = 'get_data.py'
    with open(script_path, 'r', encoding='utf-8') as f:
        text = f.read()

    file_in_folder = next((script for script in my_folder if script.name == 'script.txt'), None)

    if file_in_folder:
        repo.update_file('Daniel/script/script.txt', f'Arquivo atualizado em {now}', text, file_in_folder.sha)
        print('Script atualizado no diretório.')
        sleep(1)
    else:
        print('script não atualizado no diretório.')

if 'doc' not in git_cont:
    doc_path = 'support_files/documentação.json'
    with open(doc_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    repo.create_file('doc/documentação.txt', f'Arquivo criado em {now}.',
                     json.dumps(data, indent=4, ensure_ascii=False))
    sleep(1)
else:
    my_folder = repo.get_contents(doc_git_path)
    doc_path = 'support_files/documentação.json'
    with open(doc_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    file_in_folder = next((doc_f for doc_f in my_folder if doc_f.name == 'documentação.json'), None)
    if file_in_folder:
        repo.update_file(f'doc/documentação.json', f'Arquivo atualizado em {now}.',
                         json.dumps(data, indent=4, ensure_ascii=False), file_in_folder.sha)
        print('Documentação atualizada no diretório.')
        sleep(1)
    else:
        print('Documentação não atualizada no diretório.')

g.close()
